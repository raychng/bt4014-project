{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alif8\\anaconda3\\envs\\main1\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Imported required packages\n",
    "from __future__ import annotations\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "from collections.abc import Iterator\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import beta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Setting plot size, plot style & random seed\n",
    "plt.rcParams[\"figure.figsize\"] = (15,8)\n",
    "plt.style.use('ggplot')\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditAlgorithm(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class to capture common behaviours among different types of bandit algorithms.\n",
    "    \n",
    "    select_arm will depend on specific bandit algorithm and is left as an abstract method.\n",
    "    \"\"\"\n",
    "    \n",
    "    def set_arms(self, n_arms: int) -> BanditAlgorithm:\n",
    "        \"\"\"\n",
    "        Method to set counts & values if previously not initialised\n",
    "        \n",
    "        :param n_arms: Number of arms in the simulation\n",
    "        :return: Self\n",
    "        \"\"\"\n",
    "        self.counts: List[int] = [0 for arm in range(n_arms + 1)]\n",
    "        self.values: List[float] = [0.0 for arm in range(n_arms + 1)]\n",
    "        return self\n",
    "    \n",
    "    @abstractmethod\n",
    "    def select_arm(self) -> int:\n",
    "        \"\"\"\n",
    "        Method to select arm\n",
    "        \n",
    "        :return: Index of chosen arm\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def update(self, chosen_arm: int, reward: float) -> BanditAlgorithm:\n",
    "        \"\"\"\n",
    "        Method to update counts & values after choosing an arm and obtaining a reward\n",
    "        \n",
    "        :param chosen_arm: Index of chosen arm\n",
    "        :param reward: Reward earned from arm\n",
    "        :return: Self\n",
    "        \"\"\"\n",
    "        self.counts[chosen_arm] += 1\n",
    "        n = self.counts[chosen_arm]\n",
    "        value = self.values[chosen_arm]\n",
    "        \n",
    "        new_value = (((n - 1) / float(n)) * value) + ((1 / float(n)) * reward)\n",
    "        self.values[chosen_arm] = new_value\n",
    "        return self\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        # Return name of algorithm, for plotting graphs\n",
    "        return self.__class__.__name__                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy(BanditAlgorithm):\n",
    "    \"\"\"\n",
    "    Epsilon-Greedy bandit algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon: float = 0.1) -> None:\n",
    "        # Default epsilon is 0.1\n",
    "        self.epsilon = epsilon\n",
    "        return\n",
    "    \n",
    "    def select_arm(self) -> int:\n",
    "        # Observe if this step will be exploration or exploitation\n",
    "        if random.random() > self.epsilon:\n",
    "            # If exploit, select best arm\n",
    "            m = max(self.values)\n",
    "            return self.values.index(m)\n",
    "        else:\n",
    "            # If explore, select random arm\n",
    "            return random.randrange(len(self.values))\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"Epsilon-Greedy with epsilon: {self.epsilon}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonDecay(BanditAlgorithm):\n",
    "    \"\"\"\n",
    "    Epsilon-Decay bandit algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def select_arm(self) -> int:\n",
    "        \n",
    "        # Calculate epsilon\n",
    "        epsilon = 1 / ((sum(self.counts) / len(self.counts)) + 1)\n",
    "        \n",
    "        # Observe if this step will be exploration or exploitation\n",
    "        if random.random() > epsilon:\n",
    "            # If exploit, select best arm\n",
    "            m = max(self.values)\n",
    "            return self.values.index(m)\n",
    "        else:\n",
    "            # If explore, select random arm\n",
    "            return random.randrange(len(self.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(BanditAlgorithm):\n",
    "    \"\"\"\n",
    "    Softmax bandit algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tau: float = 0.1) -> None:\n",
    "        # Default temperature is 0.1\n",
    "        self.tau = tau\n",
    "        return\n",
    "    \n",
    "    def select_arm(self) -> int:\n",
    "        # Calcualte proability\n",
    "        z = sum([math.exp(v / self.tau) for v in self.values])\n",
    "        pi = [math.exp(v / self.tau) / z for v in self.values]\n",
    "        \n",
    "        # Draw arm based on probability\n",
    "        choice = np.random.choice(len(self.values), p=pi)\n",
    "        return choice\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"Softmax with temperature: {self.tau}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnealingSoftmax(BanditAlgorithm):\n",
    "    \"\"\"\n",
    "    Annealing softmax bandit algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def select_arm(self) -> int:\n",
    "        \n",
    "        # Calculate temperature\n",
    "        tau = 1 / ((sum(self.counts) / len(self.counts)) + 1)\n",
    "        \n",
    "        # Calcualte proability\n",
    "        z = sum([math.exp(v / tau) for v in self.values])\n",
    "        pi = [math.exp(v / tau) / z for v in self.values]\n",
    "        \n",
    "        # Draw arm based on probability\n",
    "        choice = np.random.choice(len(self.values), p=pi)\n",
    "        return choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB(BanditAlgorithm):\n",
    "    \"\"\"\n",
    "    Upper confidence bound bandit algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def select_arm(self) -> int:\n",
    "        # Ensure that each arm has been played at least once\n",
    "        for arm, count in enumerate(self.counts):\n",
    "            if count == 0:\n",
    "                return arm\n",
    "        \n",
    "        # Calculate the ucb of each arm\n",
    "        ucb_values = [value for value in self.values]\n",
    "        total_count = sum(self.counts)\n",
    "        for arm in range(len(self.values)):\n",
    "            # Calculate curiousity bonus which allows for exploration\n",
    "            bonus = math.sqrt(\n",
    "                (2 * math.log(total_count))\n",
    "                / (float(self.counts[arm]))\n",
    "            )\n",
    "            ucb_values[arm] += bonus\n",
    "        \n",
    "        # Return arm with highest ucb\n",
    "        return ucb_values.index(max(ucb_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaBandit(BanditAlgorithm):\n",
    "    \"\"\"\n",
    "    Abstract bandit algorithm class for BayesianUCB & Thompson Sampling, both utilise Beta distribution for prior distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self\n",
    "                 , prior_alpha: float = 1.0\n",
    "                 , prior_beta: float = 1.0\n",
    "                 , stdnum: float = 3.0\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.prior_alpha = prior_alpha\n",
    "        self.prior_beta = prior_beta\n",
    "        return\n",
    "    \n",
    "    def set_arms(self, n_arms: int) -> BanditAlgorithm:\n",
    "        self.alphas = [self.prior_alpha] * n_arms\n",
    "        self.betas = [self.prior_beta] * n_arms\n",
    "        super().set_arms(n_arms)\n",
    "        return\n",
    "    \n",
    "    def update(self, chosen_arm: int, reward: float) -> BanditAlgorithm:\n",
    "        # Updating counts & values\n",
    "        super().update(chosen_arm, reward)\n",
    "        # Updating alphas & betas\n",
    "        self.alphas[chosen_arm] += reward\n",
    "        self.betas[chosen_arm] += (1 - reward)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianUCB(BetaBandit):\n",
    "    \"\"\"\n",
    "    Bayesian upper confidence bound bandit algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self\n",
    "                 , prior_alpha: float = 1.0\n",
    "                 , prior_beta: float = 1.0\n",
    "                 , stdnum: float = 3.0\n",
    "                 ) -> None:\n",
    "        super().__init__(prior_alpha, prior_beta)\n",
    "        self.stdnum = stdnum\n",
    "        return\n",
    "    \n",
    "    def select_arm(self) -> int:\n",
    "        # Calculate the ucb of each arm\n",
    "        ucb_values = [0.0 for arm in self.values]\n",
    "        for arm in range(len(self.values)):\n",
    "            # Calculate mean of beta distribution\n",
    "            expected_reward = self.alphas[arm] / float(self.alphas[arm] + self.betas[arm])\n",
    "            # Calculate curiousity bonus which allows for exploration\n",
    "            bonus = beta.std(self.alphas[arm], self.betas[arm]) * self.stdnum\n",
    "            ucb_values[arm] = expected_reward + bonus\n",
    "        \n",
    "        # Return arm with highest ucb\n",
    "        return ucb_values.index(max(ucb_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSampling(BetaBandit):\n",
    "    \"\"\"\n",
    "    Thompson sampling bandit algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def select_arm(self) -> int:\n",
    "        # Calculate theta values for each arm\n",
    "        theta_values = [0.0 for arm in self.values]\n",
    "        for arm in range(len(self.values)):\n",
    "            # Sample from beta distribution\n",
    "            theta_values[arm] = np.random.beta(self.alphas[arm], self.betas[arm])\n",
    "        \n",
    "        # Return arm with highest theta\n",
    "        return theta_values.index(max(theta_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualBandit(BanditAlgorithm):\n",
    "    \"\"\"\n",
    "    Abstract class for contextual bandits\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinUCB(ContextualBandit):\n",
    "    \"\"\"\n",
    "    Linear UCB contextual bandit algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def set_arms(self\n",
    "                 , n_arms: int\n",
    "                 , n_features: int\n",
    "                 , alpha: float = 0.1) -> BanditAlgorithm:\n",
    "        self.alpha = alpha\n",
    "        self.n_features = n_features\n",
    "        self.A = [np.identity(n_features) for _ in range(n_arms)]\n",
    "        self.b = [np.zeros(n_features) for _ in range(n_arms)]\n",
    "        return\n",
    "    \n",
    "    def select_arm(self, context: np.ndarray) -> int:\n",
    "        # Initialise scores\n",
    "        scores = np.zeroes(len(self.A))\n",
    "        for arm in range(len(self.A)):\n",
    "            # Calculate score for each arm\n",
    "            A_inv = np.linalg.inv(self.A[arm])\n",
    "            theta = A_inv @ self.b[arm]\n",
    "            p_arm = ((theta.transpose() @ context) \n",
    "                     + (self.alpha \n",
    "                        * np.sqrt(context.transpose() @ A_inv @ context))\n",
    "            )\n",
    "            scores[arm] = p_arm\n",
    "        return np.argmax(scores)\n",
    "    \n",
    "    def update(self\n",
    "               , chosen_arm: int\n",
    "               , reward: float\n",
    "               , context: np.ndarray\n",
    "               ) -> BanditAlgorithm:\n",
    "        self.A[chosen_arm] += np.outer(context, context)\n",
    "        self.B[chosen_arm] += reward * context\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_replay(\n",
    "    algos: List[BanditAlgorithm]\n",
    "    , data: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to perform evaluation replay with multiple bandit algorithms, including contextual bandits\n",
    "    \n",
    "    Results will contain policy, reward, cumulative reward for each time step\n",
    "    \n",
    "    :param algos: List of bandit algorithms to run\n",
    "    :param data: DataFrame with 'business_id' as arm, 'stars' as reward\n",
    "    :return: DataFrame containg results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Count number of reviews\n",
    "    n = data.shape[0]\n",
    "    # Count number of arms\n",
    "    n_arms = data['business_id'].nunique()\n",
    "    results = []\n",
    "    # Excluding all non-context columns\n",
    "    context_df = data[data.columns[~data.columns.isin([\n",
    "        'review_id'\n",
    "        , 'user_id'\n",
    "        , 'business_id'\n",
    "        , 'stars'\n",
    "        , 'date'\n",
    "        , 'text'\n",
    "        , 'useful'\n",
    "        , 'funny'\n",
    "        , 'cool'\n",
    "    ])]]\n",
    "    # Count number of features\n",
    "    n_features = context_df.shape[1]\n",
    "    \n",
    "    for algo in algos:\n",
    "        # Initialising arms\n",
    "        if isinstance(algo, ContextualBandit):\n",
    "            algo.set_arms(n_arms, n_features)\n",
    "        else:\n",
    "            algo.set_arms(n_arms)\n",
    "        \n",
    "        cumulative_reward = 0.0\n",
    "        for t in range(n):\n",
    "            # Extract context at current time step\n",
    "            context = context_df.iloc[t].to_numpy()\n",
    "            \n",
    "            # Draw arm\n",
    "            if isinstance(algo, ContextualBandit):\n",
    "                chosen_arm = algo.select_arm(context)\n",
    "            else:\n",
    "                chosen_arm = algo.select_arm()\n",
    "            \n",
    "            # Compare to actual arm\n",
    "            actual_arm = data.iloc[t]['business_id']\n",
    "            if chosen_arm == actual_arm:\n",
    "                # Update algorithm with reward if chosen arm matches actual arm\n",
    "                reward = data.iloc[t]['stars']\n",
    "                if isinstance(algo, ContextualBandit):\n",
    "                    algo.update(chosen_arm, reward, context)\n",
    "                else:\n",
    "                    algo.update(chosen_arm, reward)\n",
    "                \n",
    "                # Updating cumulative reward\n",
    "                cumulative_reward += reward\n",
    "                \n",
    "                # Appending results at current step\n",
    "                current_step = {\n",
    "                    'policy': str(algo),\n",
    "                    'time_step': t,\n",
    "                    'reward': reward,\n",
    "                    'cumulative_reward': cumulative_reward\n",
    "                }\n",
    "                results.append(current_step)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_results(data: pd.DataFrame, target_metric: str) -> None:\n",
    "    \"\"\"\n",
    "    Helper function to plot results, x-axis will be time_step\n",
    "    \n",
    "    :param data: DataFrame containg results of simulation\n",
    "    :param target_metric: Metric to plot on y-axis\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Collect names of algorithm\n",
    "    algo_names = data['policy'].unique().tolist()\n",
    "    for algo_name in algo_names:\n",
    "        # Filter results for that algorithm\n",
    "        algo_result = data[data['policy'] == algo_name]\n",
    "        ax.plot(\n",
    "            algo_result['time_step']\n",
    "            , algo_result[target_metric]\n",
    "            , label=algo_name)\n",
    "    plt.legend()\n",
    "    plt.title(f\"{target_metric} at each step\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
